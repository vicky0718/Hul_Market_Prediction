{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502e604f",
   "metadata": {},
   "source": [
    "# Simple Baseline Models for S&P 500 Prediction\n",
    "\n",
    "## Overview\n",
    "This notebook implements baseline models with proper model selection using the original train.csv and test.csv files.\n",
    "\n",
    "**Approach:**\n",
    "- Use original data without advanced imputation\n",
    "- Implement robust model selection with cross-validation\n",
    "- Train on train.csv and evaluate on test.csv\n",
    "- Focus on interpretable baseline performance\n",
    "- Include competition-specific metrics\n",
    "\n",
    "**Models:**\n",
    "- Linear models: Ridge, Lasso, ElasticNet\n",
    "- Tree-based: RandomForest, GradientBoosting\n",
    "- Advanced: XGBoost, LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc354ed",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2746180f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced models (XGBoost, LightGBM) available\n",
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Advanced models (with fallback)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    import lightgbm as lgb\n",
    "    ADVANCED_MODELS = True\n",
    "    print(\"âœ… Advanced models (XGBoost, LightGBM) available\")\n",
    "except ImportError:\n",
    "    ADVANCED_MODELS = False\n",
    "    print(\"âš ï¸ Advanced models not available - using sklearn models only\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5453a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loading datasets...\n",
      "âœ… Data loaded successfully:\n",
      "   â€¢ Training data: (8990, 98)\n",
      "   â€¢ Test data: (10, 99)\n",
      "   â€¢ Training missing values: 137,675\n",
      "   â€¢ Test missing values: 0\n",
      "   â€¢ Available targets: ['forward_returns', 'market_forward_excess_returns', 'risk_free_rate']\n",
      "   â€¢ Using primary target: forward_returns\n",
      "âœ… Data loaded successfully:\n",
      "   â€¢ Training data: (8990, 98)\n",
      "   â€¢ Test data: (10, 99)\n",
      "   â€¢ Training missing values: 137,675\n",
      "   â€¢ Test missing values: 0\n",
      "   â€¢ Available targets: ['forward_returns', 'market_forward_excess_returns', 'risk_free_rate']\n",
      "   â€¢ Using primary target: forward_returns\n"
     ]
    }
   ],
   "source": [
    "# Load training and test data\n",
    "print(\"ğŸ“Š Loading datasets...\")\n",
    "df_train = pd.read_csv('../data/raw/train.csv')\n",
    "df_test = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "print(f\"âœ… Data loaded successfully:\")\n",
    "print(f\"   â€¢ Training data: {df_train.shape}\")\n",
    "print(f\"   â€¢ Test data: {df_test.shape}\")\n",
    "print(f\"   â€¢ Training missing values: {df_train.isnull().sum().sum():,}\")\n",
    "print(f\"   â€¢ Test missing values: {df_test.isnull().sum().sum():,}\")\n",
    "\n",
    "# Check for target variable\n",
    "target_cols = ['forward_returns', 'market_forward_excess_returns', 'risk_free_rate']\n",
    "available_targets = [col for col in target_cols if col in df_train.columns]\n",
    "print(f\"   â€¢ Available targets: {available_targets}\")\n",
    "\n",
    "if 'forward_returns' in df_train.columns:\n",
    "    primary_target = 'forward_returns'\n",
    "    print(f\"   â€¢ Using primary target: {primary_target}\")\n",
    "else:\n",
    "    primary_target = available_targets[0] if available_targets else None\n",
    "    print(f\"   â€¢ Using fallback target: {primary_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e2500",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0eb9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ DATA PREPROCESSING\n",
      "==================================================\n",
      "ğŸ“Š Feature Analysis:\n",
      "   â€¢ Total features: 94\n",
      "   â€¢ Excluded columns: ['date_id', 'forward_returns', 'market_forward_excess_returns', 'risk_free_rate']\n",
      "\n",
      "ğŸ“ˆ Missing Value Categories:\n",
      "   â€¢ Complete features (0% missing): 9\n",
      "   â€¢ Low missing (1-10%): 0\n",
      "   â€¢ Medium missing (11-50%): 77\n",
      "   â€¢ High missing (>50%): 8\n",
      "\n",
      "âœ… Complete features: ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9']\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”§ DATA PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify feature columns (exclude targets and IDs)\n",
    "exclude_cols = ['date_id', 'forward_returns', 'market_forward_excess_returns', 'risk_free_rate']\n",
    "feature_cols = [col for col in df_train.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"ğŸ“Š Feature Analysis:\")\n",
    "print(f\"   â€¢ Total features: {len(feature_cols)}\")\n",
    "print(f\"   â€¢ Excluded columns: {exclude_cols}\")\n",
    "\n",
    "# Analyze missing values in features\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'column': feature_cols,\n",
    "    'missing_count': [df_train[col].isnull().sum() for col in feature_cols],\n",
    "    'missing_pct': [df_train[col].isnull().sum() / len(df_train) * 100 for col in feature_cols]\n",
    "})\n",
    "missing_analysis = missing_analysis.sort_values('missing_pct')\n",
    "\n",
    "# Categorize features by missing value percentage\n",
    "complete_features = missing_analysis[missing_analysis['missing_pct'] == 0]['column'].tolist()\n",
    "low_missing = missing_analysis[(missing_analysis['missing_pct'] > 0) & (missing_analysis['missing_pct'] <= 10)]['column'].tolist()\n",
    "medium_missing = missing_analysis[(missing_analysis['missing_pct'] > 10) & (missing_analysis['missing_pct'] <= 50)]['column'].tolist()\n",
    "high_missing = missing_analysis[missing_analysis['missing_pct'] > 50]['column'].tolist()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Missing Value Categories:\")\n",
    "print(f\"   â€¢ Complete features (0% missing): {len(complete_features)}\")\n",
    "print(f\"   â€¢ Low missing (1-10%): {len(low_missing)}\")\n",
    "print(f\"   â€¢ Medium missing (11-50%): {len(medium_missing)}\")\n",
    "print(f\"   â€¢ High missing (>50%): {len(high_missing)}\")\n",
    "\n",
    "# Show complete features (most reliable for baseline)\n",
    "if len(complete_features) > 0:\n",
    "    print(f\"\\nâœ… Complete features: {complete_features[:10]}{'...' if len(complete_features) > 10 else ''}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ No complete features found - will use simple imputation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6d627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ FEATURE SET CREATION\n",
      "==================================================\n",
      "âœ… Feature Set 1: Complete features only (9 features)\n",
      "âœ… Feature Set 2: Reliable features (â‰¤10% missing) (9 features)\n",
      "âœ… Feature Set 3: Extended features (â‰¤50% missing) (86 features)\n",
      "âœ… Feature Set 4: All features with imputation (94 features)\n",
      "\n",
      "ğŸ“Š Created 4 feature sets for model selection\n",
      "   â€¢ complete_only: 9 features\n",
      "   â€¢ reliable: 9 features\n",
      "   â€¢ extended: 86 features\n",
      "   â€¢ all_features: 94 features\n"
     ]
    }
   ],
   "source": [
    "# Create different feature sets for model selection\n",
    "print(\"\\nğŸ¯ FEATURE SET CREATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Strategy 1: Use only complete features\n",
    "feature_sets = {}\n",
    "\n",
    "if len(complete_features) >= 5:\n",
    "    feature_sets['complete_only'] = complete_features\n",
    "    print(f\"âœ… Feature Set 1: Complete features only ({len(complete_features)} features)\")\n",
    "\n",
    "# Strategy 2: Include low missing value features with imputation\n",
    "reliable_features = complete_features + low_missing\n",
    "if len(reliable_features) >= 5:\n",
    "    feature_sets['reliable'] = reliable_features\n",
    "    print(f\"âœ… Feature Set 2: Reliable features (â‰¤10% missing) ({len(reliable_features)} features)\")\n",
    "\n",
    "# Strategy 3: Include medium missing with imputation\n",
    "extended_features = complete_features + low_missing + medium_missing\n",
    "if len(extended_features) >= 5:\n",
    "    feature_sets['extended'] = extended_features\n",
    "    print(f\"âœ… Feature Set 3: Extended features (â‰¤50% missing) ({len(extended_features)} features)\")\n",
    "\n",
    "# Strategy 4: All features with aggressive imputation\n",
    "if len(feature_cols) >= 5:\n",
    "    feature_sets['all_features'] = feature_cols\n",
    "    print(f\"âœ… Feature Set 4: All features with imputation ({len(feature_cols)} features)\")\n",
    "\n",
    "# Fallback: Use top features by correlation if no good sets\n",
    "if len(feature_sets) == 0:\n",
    "    print(\"âš ï¸ Creating fallback feature set based on correlation...\")\n",
    "    \n",
    "    # Simple correlation analysis with target\n",
    "    if primary_target in df_train.columns:\n",
    "        target_corr = df_train[feature_cols + [primary_target]].corr()[primary_target].abs().sort_values(ascending=False)\n",
    "        top_corr_features = target_corr.drop(primary_target).head(10).index.tolist()\n",
    "        feature_sets['top_correlated'] = top_corr_features\n",
    "        print(f\"âœ… Fallback: Top correlated features ({len(top_corr_features)} features)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Created {len(feature_sets)} feature sets for model selection\")\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"   â€¢ {name}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471b1a4",
   "metadata": {},
   "source": [
    "## 3. Model Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1b3f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– MODEL CONFIGURATION\n",
      "==================================================\n",
      "âœ… Configured 7 baseline models:\n",
      "   â€¢ Ridge\n",
      "   â€¢ Lasso\n",
      "   â€¢ ElasticNet\n",
      "   â€¢ RandomForest\n",
      "   â€¢ GradientBoosting\n",
      "   â€¢ XGBoost\n",
      "   â€¢ LightGBM\n",
      "\n",
      "ğŸ“Š Cross-validation: TimeSeriesSplit with 5 folds\n",
      "   â€¢ Preserves temporal order in financial data\n",
      "   â€¢ Avoids look-ahead bias\n"
     ]
    }
   ],
   "source": [
    "# Define baseline models\n",
    "print(\"ğŸ¤– MODEL CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configure models with reasonable parameters\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': Lasso(alpha=0.1, random_state=42, max_iter=2000),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=2000),\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingRegressor(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=6, \n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Add advanced models if available\n",
    "if ADVANCED_MODELS:\n",
    "    models['XGBoost'] = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    models['LightGBM'] = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Configured {len(models)} baseline models:\")\n",
    "for name in models.keys():\n",
    "    print(f\"   â€¢ {name}\")\n",
    "\n",
    "# Configure cross-validation strategy\n",
    "# Use TimeSeriesSplit for financial data\n",
    "cv_strategy = TimeSeriesSplit(n_splits=5)\n",
    "print(f\"\\nğŸ“Š Cross-validation: TimeSeriesSplit with 5 folds\")\n",
    "print(f\"   â€¢ Preserves temporal order in financial data\")\n",
    "print(f\"   â€¢ Avoids look-ahead bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00c6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for evaluation\n",
    "def calculate_hit_rate(y_true, y_pred):\n",
    "    \"\"\"Calculate hit rate (directional accuracy)\"\"\"\n",
    "    return np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.0):\n",
    "    \"\"\"Calculate Sharpe ratio\"\"\"\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    if np.std(excess_returns) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    try:\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'train_r2': r2_score(y_train, y_pred_train),\n",
    "            'test_r2': r2_score(y_test, y_pred_test),\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "            'hit_rate': calculate_hit_rate(y_test, y_pred_test),\n",
    "            'sharpe_ratio': calculate_sharpe_ratio(y_pred_test),\n",
    "            'overfitting': abs(results.get('train_r2', 0) - results.get('test_r2', 0)) if 'train_r2' in locals() else 0\n",
    "        }\n",
    "        \n",
    "        # Fix overfitting calculation\n",
    "        results['overfitting'] = abs(results['train_r2'] - results['test_r2'])\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'error': str(e),\n",
    "            'train_r2': np.nan,\n",
    "            'test_r2': np.nan,\n",
    "            'train_rmse': np.nan,\n",
    "            'test_rmse': np.nan,\n",
    "            'hit_rate': np.nan,\n",
    "            'sharpe_ratio': np.nan,\n",
    "            'overfitting': np.nan\n",
    "        }\n",
    "\n",
    "print(\"âœ… Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caff36a",
   "metadata": {},
   "source": [
    "## 4. Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de65bc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ MODEL SELECTION AND TRAINING\n",
      "============================================================\n",
      "ğŸ¯ Target variable: forward_returns\n",
      "   â€¢ Training samples: 8,990\n",
      "   â€¢ Target missing values: 0\n",
      "   â€¢ Final training samples: 8,990\n",
      "\n",
      "ğŸ“Š TESTING FEATURE SET: COMPLETE_ONLY\n",
      "--------------------------------------------------\n",
      "   â€¢ Features: 9 available\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   ğŸ”„ Ridge... âœ… RÂ²=-0.0020, Hit=0.516\n",
      "   ğŸ”„ Lasso... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ ElasticNet... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ RandomForest... âœ… RÂ²=-0.0039, Hit=0.538\n",
      "   ğŸ”„ GradientBoosting... âœ… RÂ²=-0.0039, Hit=0.538\n",
      "   ğŸ”„ GradientBoosting... âœ… RÂ²=-0.0036, Hit=0.536\n",
      "   ğŸ”„ XGBoost... âœ… RÂ²=-0.0029, Hit=0.537\n",
      "   ğŸ”„ LightGBM... âœ… RÂ²=-0.0036, Hit=0.536\n",
      "   ğŸ”„ XGBoost... âœ… RÂ²=-0.0029, Hit=0.537\n",
      "   ğŸ”„ LightGBM... âœ… RÂ²=-0.0039, Hit=0.537\n",
      "   ğŸ† Best in set: Lasso (RÂ²=-0.0003)\n",
      "\n",
      "ğŸ“Š TESTING FEATURE SET: RELIABLE\n",
      "--------------------------------------------------\n",
      "   â€¢ Features: 9 available\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   ğŸ”„ Ridge... âœ… RÂ²=-0.0020, Hit=0.516\n",
      "   ğŸ”„ Lasso... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ ElasticNet... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ RandomForest... âœ… RÂ²=-0.0039, Hit=0.537\n",
      "   ğŸ† Best in set: Lasso (RÂ²=-0.0003)\n",
      "\n",
      "ğŸ“Š TESTING FEATURE SET: RELIABLE\n",
      "--------------------------------------------------\n",
      "   â€¢ Features: 9 available\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   ğŸ”„ Ridge... âœ… RÂ²=-0.0020, Hit=0.516\n",
      "   ğŸ”„ Lasso... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ ElasticNet... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ RandomForest... âœ… RÂ²=-0.0039, Hit=0.538\n",
      "   ğŸ”„ GradientBoosting... âœ… RÂ²=-0.0039, Hit=0.538\n",
      "   ğŸ”„ GradientBoosting... âœ… RÂ²=-0.0036, Hit=0.536\n",
      "   ğŸ”„ XGBoost... âœ… RÂ²=-0.0029, Hit=0.537\n",
      "   ğŸ”„ LightGBM... âœ… RÂ²=-0.0036, Hit=0.536\n",
      "   ğŸ”„ XGBoost... âœ… RÂ²=-0.0029, Hit=0.537\n",
      "   ğŸ”„ LightGBM... âœ… RÂ²=-0.0039, Hit=0.537\n",
      "   ğŸ† Best in set: Lasso (RÂ²=-0.0003)\n",
      "\n",
      "ğŸ“Š TESTING FEATURE SET: EXTENDED\n",
      "--------------------------------------------------\n",
      "   â€¢ Features: 86 available\n",
      "âœ… RÂ²=-0.0039, Hit=0.537\n",
      "   ğŸ† Best in set: Lasso (RÂ²=-0.0003)\n",
      "\n",
      "ğŸ“Š TESTING FEATURE SET: EXTENDED\n",
      "--------------------------------------------------\n",
      "   â€¢ Features: 86 available\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   ğŸ”„ Ridge... âœ… RÂ²=-0.1194, Hit=0.546\n",
      "   ğŸ”„ Lasso... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ ElasticNet... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ RandomForest...    â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   ğŸ”„ Ridge... âœ… RÂ²=-0.1194, Hit=0.546\n",
      "   ğŸ”„ Lasso... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ ElasticNet... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ RandomForest... âœ… RÂ²=-0.0417, Hit=0.532\n",
      "   ğŸ”„ GradientBoosting... âœ… RÂ²=-0.0417, Hit=0.532\n",
      "   ğŸ”„ GradientBoosting... âœ… RÂ²=-0.1969, Hit=0.497\n",
      "   ğŸ”„ XGBoost... âœ… RÂ²=-0.1969, Hit=0.497\n",
      "   ğŸ”„ XGBoost... âœ… RÂ²=-0.1487, Hit=0.498\n",
      "   ğŸ”„ LightGBM... âœ… RÂ²=-0.1487, Hit=0.498\n",
      "   ğŸ”„ LightGBM... âœ… RÂ²=-0.1376, Hit=0.496\n",
      "   ğŸ† Best in set: Lasso (RÂ²=-0.0003)\n",
      "\n",
      "ğŸ“Š TESTING FEATURE SET: ALL_FEATURES\n",
      "--------------------------------------------------\n",
      "   â€¢ Features: 94 available\n",
      "âœ… RÂ²=-0.1376, Hit=0.496\n",
      "   ğŸ† Best in set: Lasso (RÂ²=-0.0003)\n",
      "\n",
      "ğŸ“Š TESTING FEATURE SET: ALL_FEATURES\n",
      "--------------------------------------------------\n",
      "   â€¢ Features: 94 available\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   ğŸ”„ Ridge... âœ… RÂ²=-0.7524, Hit=0.551\n",
      "   ğŸ”„ Lasso... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ ElasticNet... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ RandomForest...    â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   ğŸ”„ Ridge... âœ… RÂ²=-0.7524, Hit=0.551\n",
      "   ğŸ”„ Lasso... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ ElasticNet... âœ… RÂ²=-0.0003, Hit=0.552\n",
      "   ğŸ”„ RandomForest... âœ… RÂ²=-0.0395, Hit=0.539\n",
      "   ğŸ”„ GradientBoosting... âœ… RÂ²=-0.0395, Hit=0.539\n",
      "   ğŸ”„ GradientBoosting... âœ… RÂ²=-0.2240, Hit=0.508\n",
      "   ğŸ”„ XGBoost... âœ… RÂ²=-0.2240, Hit=0.508\n",
      "   ğŸ”„ XGBoost... âœ… RÂ²=-0.2266, Hit=0.497\n",
      "   ğŸ”„ LightGBM... âœ… RÂ²=-0.2266, Hit=0.497\n",
      "   ğŸ”„ LightGBM... âœ… RÂ²=-0.2134, Hit=0.487\n",
      "   ğŸ† Best in set: Lasso (RÂ²=-0.0003)\n",
      "\n",
      "âœ… Model selection complete! Tested 28 model configurations.\n",
      "âœ… RÂ²=-0.2134, Hit=0.487\n",
      "   ğŸ† Best in set: Lasso (RÂ²=-0.0003)\n",
      "\n",
      "âœ… Model selection complete! Tested 28 model configurations.\n"
     ]
    }
   ],
   "source": [
    "# Main model selection and training loop\n",
    "print(\"ğŸš€ MODEL SELECTION AND TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if primary_target is None:\n",
    "    print(\"âŒ No target variable found! Cannot proceed with training.\")\n",
    "else:\n",
    "    # Prepare target variable\n",
    "    y_train_full = df_train[primary_target].copy()\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = []\n",
    "    best_model_info = {'score': -np.inf, 'model': None, 'features': None, 'scaler': None}\n",
    "    \n",
    "    print(f\"ğŸ¯ Target variable: {primary_target}\")\n",
    "    print(f\"   â€¢ Training samples: {len(y_train_full):,}\")\n",
    "    print(f\"   â€¢ Target missing values: {y_train_full.isnull().sum()}\")\n",
    "    \n",
    "    # Remove rows with missing targets\n",
    "    valid_idx = ~y_train_full.isnull()\n",
    "    if valid_idx.sum() < len(y_train_full):\n",
    "        print(f\"   â€¢ Removing {(~valid_idx).sum()} samples with missing targets\")\n",
    "        df_train_clean = df_train[valid_idx].copy()\n",
    "        y_train_full = y_train_full[valid_idx].copy()\n",
    "    else:\n",
    "        df_train_clean = df_train.copy()\n",
    "    \n",
    "    print(f\"   â€¢ Final training samples: {len(y_train_full):,}\")\n",
    "    \n",
    "    # Test each feature set\n",
    "    for set_name, feature_list in feature_sets.items():\n",
    "        print(f\"\\nğŸ“Š TESTING FEATURE SET: {set_name.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Get features that exist in both train and test\n",
    "            available_features = [f for f in feature_list if f in df_train_clean.columns and f in df_test.columns]\n",
    "            \n",
    "            if len(available_features) == 0:\n",
    "                print(f\"   âŒ No valid features found for {set_name}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"   â€¢ Features: {len(available_features)} available\")\n",
    "            \n",
    "            # Prepare feature data\n",
    "            X_train_raw = df_train_clean[available_features].copy()\n",
    "            X_test_raw = df_test[available_features].copy()\n",
    "            \n",
    "            # Handle missing values with simple imputation\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            X_train_imputed = pd.DataFrame(\n",
    "                imputer.fit_transform(X_train_raw),\n",
    "                columns=available_features,\n",
    "                index=X_train_raw.index\n",
    "            )\n",
    "            X_test_imputed = pd.DataFrame(\n",
    "                imputer.transform(X_test_raw),\n",
    "                columns=available_features,\n",
    "                index=X_test_raw.index\n",
    "            )\n",
    "            \n",
    "            # Check for remaining missing values\n",
    "            train_missing = X_train_imputed.isnull().sum().sum()\n",
    "            test_missing = X_test_imputed.isnull().sum().sum()\n",
    "            \n",
    "            if train_missing > 0 or test_missing > 0:\n",
    "                print(f\"   âš ï¸ Remaining missing values: Train={train_missing}, Test={test_missing}\")\n",
    "                # Forward fill any remaining missing values\n",
    "                X_train_imputed = X_train_imputed.fillna(method='ffill').fillna(0)\n",
    "                X_test_imputed = X_test_imputed.fillna(method='ffill').fillna(0)\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "            X_test_scaled = scaler.transform(X_test_imputed)\n",
    "            \n",
    "            # Create train/validation split maintaining time order\n",
    "            split_idx = int(len(X_train_scaled) * 0.8)\n",
    "            \n",
    "            X_train_split = X_train_scaled[:split_idx]\n",
    "            X_val_split = X_train_scaled[split_idx:]\n",
    "            y_train_split = y_train_full.iloc[:split_idx]\n",
    "            y_val_split = y_train_full.iloc[split_idx:]\n",
    "            \n",
    "            print(f\"   â€¢ Train split: {len(X_train_split):,} samples\")\n",
    "            print(f\"   â€¢ Validation split: {len(X_val_split):,} samples\")\n",
    "            \n",
    "            # Test each model\n",
    "            set_results = []\n",
    "            for model_name, model in models.items():\n",
    "                print(f\"   ğŸ”„ {model_name}...\", end=\" \")\n",
    "                \n",
    "                result = evaluate_model(\n",
    "                    model, X_train_split, y_train_split, \n",
    "                    X_val_split, y_val_split, model_name\n",
    "                )\n",
    "                result['feature_set'] = set_name\n",
    "                result['n_features'] = len(available_features)\n",
    "                \n",
    "                if 'error' in result:\n",
    "                    print(f\"âŒ Failed: {result['error'][:50]}\")\n",
    "                else:\n",
    "                    print(f\"âœ… RÂ²={result['test_r2']:.4f}, Hit={result['hit_rate']:.3f}\")\n",
    "                    \n",
    "                    # Track best model\n",
    "                    if result['test_r2'] > best_model_info['score']:\n",
    "                        best_model_info.update({\n",
    "                            'score': result['test_r2'],\n",
    "                            'model': model,\n",
    "                            'model_name': model_name,\n",
    "                            'features': available_features,\n",
    "                            'scaler': scaler,\n",
    "                            'feature_set': set_name\n",
    "                        })\n",
    "                \n",
    "                set_results.append(result)\n",
    "                all_results.append(result)\n",
    "            \n",
    "            # Summary for this feature set\n",
    "            valid_results = [r for r in set_results if 'error' not in r]\n",
    "            if valid_results:\n",
    "                best_in_set = max(valid_results, key=lambda x: x['test_r2'])\n",
    "                print(f\"   ğŸ† Best in set: {best_in_set['model']} (RÂ²={best_in_set['test_r2']:.4f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Feature set {set_name} failed: {str(e)[:100]}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Model selection complete! Tested {len(all_results)} model configurations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c5a46",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81dfa2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š RESULTS ANALYSIS\n",
      "============================================================\n",
      "âœ… Successfully trained 28 models\n",
      "\n",
      "ğŸ“ˆ PERFORMANCE SUMMARY:\n",
      "   â€¢ Average RÂ²: -0.0762\n",
      "   â€¢ Best RÂ²: -0.0003\n",
      "   â€¢ Worst RÂ²: -0.7524\n",
      "   â€¢ Average Hit Rate: 0.532\n",
      "   â€¢ Best Hit Rate: 0.552\n",
      "\n",
      "ğŸ† TOP 5 MODELS:\n",
      "----------------------------------------\n",
      "   1. Lasso (complete_only)\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "      Features: 9, RMSE: 0.011091\n",
      "      Overfitting: 0.0003\n",
      "\n",
      "   2. ElasticNet (complete_only)\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "      Features: 9, RMSE: 0.011091\n",
      "      Overfitting: 0.0003\n",
      "\n",
      "   3. Lasso (reliable)\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "      Features: 9, RMSE: 0.011091\n",
      "      Overfitting: 0.0003\n",
      "\n",
      "   4. ElasticNet (reliable)\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "      Features: 9, RMSE: 0.011091\n",
      "      Overfitting: 0.0003\n",
      "\n",
      "   5. Lasso (extended)\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "      Features: 86, RMSE: 0.011091\n",
      "      Overfitting: 0.0003\n",
      "\n",
      "ğŸ“Š MODEL TYPE COMPARISON:\n",
      "------------------------------\n",
      "   ElasticNet:\n",
      "      Avg RÂ²: -0.0003 (Best: -0.0003)\n",
      "      Avg Hit Rate: 0.552\n",
      "      Avg Overfitting: 0.0003\n",
      "      Configurations: 4\n",
      "\n",
      "   Lasso:\n",
      "      Avg RÂ²: -0.0003 (Best: -0.0003)\n",
      "      Avg Hit Rate: 0.552\n",
      "      Avg Overfitting: 0.0003\n",
      "      Configurations: 4\n",
      "\n",
      "   RandomForest:\n",
      "      Avg RÂ²: -0.0223 (Best: -0.0039)\n",
      "      Avg Hit Rate: 0.537\n",
      "      Avg Overfitting: 0.1277\n",
      "      Configurations: 4\n",
      "\n",
      "   LightGBM:\n",
      "      Avg RÂ²: -0.0897 (Best: -0.0039)\n",
      "      Avg Hit Rate: 0.514\n",
      "      Avg Overfitting: 0.3129\n",
      "      Configurations: 4\n",
      "\n",
      "   XGBoost:\n",
      "      Avg RÂ²: -0.0953 (Best: -0.0029)\n",
      "      Avg Hit Rate: 0.517\n",
      "      Avg Overfitting: 0.3715\n",
      "      Configurations: 4\n",
      "\n",
      "   GradientBoosting:\n",
      "      Avg RÂ²: -0.1070 (Best: -0.0036)\n",
      "      Avg Hit Rate: 0.519\n",
      "      Avg Overfitting: 0.3677\n",
      "      Configurations: 4\n",
      "\n",
      "   Ridge:\n",
      "      Avg RÂ²: -0.2189 (Best: -0.0020)\n",
      "      Avg Hit Rate: 0.532\n",
      "      Avg Overfitting: 0.2361\n",
      "      Configurations: 4\n",
      "\n",
      "ğŸ”§ FEATURE SET COMPARISON:\n",
      "------------------------------\n",
      "   complete_only (9 features):\n",
      "      Avg RÂ²: -0.0024 (Best: -0.0003)\n",
      "      Avg Hit Rate: 0.538\n",
      "\n",
      "   reliable (9 features):\n",
      "      Avg RÂ²: -0.0024 (Best: -0.0003)\n",
      "      Avg Hit Rate: 0.538\n",
      "\n",
      "   extended (86 features):\n",
      "      Avg RÂ²: -0.0921 (Best: -0.0003)\n",
      "      Avg Hit Rate: 0.525\n",
      "\n",
      "   all_features (94 features):\n",
      "      Avg RÂ²: -0.2081 (Best: -0.0003)\n",
      "      Avg Hit Rate: 0.527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze results and select best model\n",
    "print(\"ğŸ“Š RESULTS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(all_results) == 0:\n",
    "    print(\"âŒ No successful model runs to analyze!\")\n",
    "else:\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame([r for r in all_results if 'error' not in r])\n",
    "    \n",
    "    if len(results_df) == 0:\n",
    "        print(\"âŒ All models failed! Check data and model configurations.\")\n",
    "    else:\n",
    "        print(f\"âœ… Successfully trained {len(results_df)} models\")\n",
    "        \n",
    "        # Overall performance summary\n",
    "        print(f\"\\nğŸ“ˆ PERFORMANCE SUMMARY:\")\n",
    "        print(f\"   â€¢ Average RÂ²: {results_df['test_r2'].mean():.4f}\")\n",
    "        print(f\"   â€¢ Best RÂ²: {results_df['test_r2'].max():.4f}\")\n",
    "        print(f\"   â€¢ Worst RÂ²: {results_df['test_r2'].min():.4f}\")\n",
    "        print(f\"   â€¢ Average Hit Rate: {results_df['hit_rate'].mean():.3f}\")\n",
    "        print(f\"   â€¢ Best Hit Rate: {results_df['hit_rate'].max():.3f}\")\n",
    "        \n",
    "        # Top 5 models\n",
    "        print(f\"\\nğŸ† TOP 5 MODELS:\")\n",
    "        print(\"-\" * 40)\n",
    "        top_models = results_df.nlargest(5, 'test_r2')\n",
    "        \n",
    "        for i, (_, row) in enumerate(top_models.iterrows(), 1):\n",
    "            print(f\"   {i}. {row['model']} ({row['feature_set']})\")\n",
    "            print(f\"      RÂ² = {row['test_r2']:.4f}, Hit Rate = {row['hit_rate']:.3f}\")\n",
    "            print(f\"      Features: {row['n_features']}, RMSE: {row['test_rmse']:.6f}\")\n",
    "            print(f\"      Overfitting: {row['overfitting']:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        # Model comparison\n",
    "        print(f\"ğŸ“Š MODEL TYPE COMPARISON:\")\n",
    "        print(\"-\" * 30)\n",
    "        model_comparison = results_df.groupby('model').agg({\n",
    "            'test_r2': ['mean', 'max', 'count'],\n",
    "            'hit_rate': 'mean',\n",
    "            'overfitting': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        model_comparison.columns = ['avg_r2', 'best_r2', 'count', 'avg_hit_rate', 'avg_overfitting']\n",
    "        model_comparison = model_comparison.sort_values('avg_r2', ascending=False)\n",
    "        \n",
    "        for model_name, stats in model_comparison.iterrows():\n",
    "            print(f\"   {model_name}:\")\n",
    "            print(f\"      Avg RÂ²: {stats['avg_r2']:.4f} (Best: {stats['best_r2']:.4f})\")\n",
    "            print(f\"      Avg Hit Rate: {stats['avg_hit_rate']:.3f}\")\n",
    "            print(f\"      Avg Overfitting: {stats['avg_overfitting']:.4f}\")\n",
    "            print(f\"      Configurations: {int(stats['count'])}\")\n",
    "            print()\n",
    "        \n",
    "        # Feature set comparison\n",
    "        print(f\"ğŸ”§ FEATURE SET COMPARISON:\")\n",
    "        print(\"-\" * 30)\n",
    "        feature_comparison = results_df.groupby('feature_set').agg({\n",
    "            'test_r2': ['mean', 'max'],\n",
    "            'hit_rate': 'mean',\n",
    "            'n_features': 'first'\n",
    "        }).round(4)\n",
    "        \n",
    "        feature_comparison.columns = ['avg_r2', 'best_r2', 'avg_hit_rate', 'n_features']\n",
    "        feature_comparison = feature_comparison.sort_values('avg_r2', ascending=False)\n",
    "        \n",
    "        for set_name, stats in feature_comparison.iterrows():\n",
    "            print(f\"   {set_name} ({int(stats['n_features'])} features):\")\n",
    "            print(f\"      Avg RÂ²: {stats['avg_r2']:.4f} (Best: {stats['best_r2']:.4f})\")\n",
    "            print(f\"      Avg Hit Rate: {stats['avg_hit_rate']:.3f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59a850",
   "metadata": {},
   "source": [
    "## 6. Final Model Training and Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a03aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ FINAL MODEL TRAINING AND TEST EVALUATION\n",
      "============================================================\n",
      "ğŸ† Selected Best Model: Lasso\n",
      "   â€¢ Feature Set: complete_only\n",
      "   â€¢ Features: 9\n",
      "   â€¢ Validation RÂ²: -0.0003\n",
      "\n",
      "ğŸ“Š Final Data Preparation:\n",
      "   â€¢ Training samples: 8,990\n",
      "   â€¢ Test samples: 10\n",
      "   â€¢ Features: 9\n",
      "   â€¢ Preprocessing complete\n",
      "\n",
      "ğŸš€ Training final model...\n",
      "\n",
      "âœ… FINAL MODEL PERFORMANCE:\n",
      "========================================\n",
      "ğŸ“Š Training Set Performance:\n",
      "   â€¢ RÂ² Score: 0.0000\n",
      "   â€¢ RMSE: 0.010550\n",
      "   â€¢ Hit Rate: 0.539\n",
      "   â€¢ Sharpe Ratio: 68699043599706112.0000\n",
      "\n",
      "ğŸ¯ Test Set Predictions Generated:\n",
      "   â€¢ Number of predictions: 10\n",
      "   â€¢ Prediction range: [0.000469, 0.000469]\n",
      "   â€¢ Mean prediction: 0.000469\n",
      "   â€¢ Std prediction: 0.000000\n",
      "\n",
      "ğŸ’¾ Predictions saved to: ../data/predictions/baseline_predictions.csv\n",
      "   â€¢ Ready for competition submission\n",
      "\n",
      "ğŸ“‹ Sample Predictions:\n",
      " date_id  forward_returns_prediction\n",
      "    8980                    0.000469\n",
      "    8981                    0.000469\n",
      "    8982                    0.000469\n",
      "    8983                    0.000469\n",
      "    8984                    0.000469\n",
      "    8985                    0.000469\n",
      "    8986                    0.000469\n",
      "    8987                    0.000469\n",
      "    8988                    0.000469\n",
      "    8989                    0.000469\n"
     ]
    }
   ],
   "source": [
    "# Train best model on full training data and evaluate on test set\n",
    "print(\"ğŸ¯ FINAL MODEL TRAINING AND TEST EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if best_model_info['model'] is None:\n",
    "    print(\"âŒ No best model found! Cannot proceed with final evaluation.\")\n",
    "else:\n",
    "    print(f\"ğŸ† Selected Best Model: {best_model_info['model_name']}\")\n",
    "    print(f\"   â€¢ Feature Set: {best_model_info['feature_set']}\")\n",
    "    print(f\"   â€¢ Features: {len(best_model_info['features'])}\")\n",
    "    print(f\"   â€¢ Validation RÂ²: {best_model_info['score']:.4f}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare final training data\n",
    "        X_train_final = df_train_clean[best_model_info['features']].copy()\n",
    "        y_train_final = y_train_full.copy()\n",
    "        \n",
    "        # Prepare test data\n",
    "        X_test_final = df_test[best_model_info['features']].copy()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Final Data Preparation:\")\n",
    "        print(f\"   â€¢ Training samples: {len(X_train_final):,}\")\n",
    "        print(f\"   â€¢ Test samples: {len(X_test_final):,}\")\n",
    "        print(f\"   â€¢ Features: {len(best_model_info['features'])}\")\n",
    "        \n",
    "        # Apply same preprocessing pipeline\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_train_imputed = pd.DataFrame(\n",
    "            imputer.fit_transform(X_train_final),\n",
    "            columns=best_model_info['features']\n",
    "        )\n",
    "        X_test_imputed = pd.DataFrame(\n",
    "            imputer.transform(X_test_final),\n",
    "            columns=best_model_info['features']\n",
    "        )\n",
    "        \n",
    "        # Handle any remaining missing values\n",
    "        X_train_imputed = X_train_imputed.fillna(method='ffill').fillna(0)\n",
    "        X_test_imputed = X_test_imputed.fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "        X_test_scaled = scaler.transform(X_test_imputed)\n",
    "        \n",
    "        print(f\"   â€¢ Preprocessing complete\")\n",
    "        \n",
    "        # Train final model on full training data\n",
    "        final_model = best_model_info['model']\n",
    "        print(f\"\\nğŸš€ Training final model...\")\n",
    "        final_model.fit(X_train_scaled, y_train_final)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_predictions = final_model.predict(X_train_scaled)\n",
    "        test_predictions = final_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        final_train_r2 = r2_score(y_train_final, train_predictions)\n",
    "        final_train_rmse = np.sqrt(mean_squared_error(y_train_final, train_predictions))\n",
    "        final_train_hit_rate = calculate_hit_rate(y_train_final, train_predictions)\n",
    "        \n",
    "        print(f\"\\nâœ… FINAL MODEL PERFORMANCE:\")\n",
    "        print(f\"=\" * 40)\n",
    "        print(f\"ğŸ“Š Training Set Performance:\")\n",
    "        print(f\"   â€¢ RÂ² Score: {final_train_r2:.4f}\")\n",
    "        print(f\"   â€¢ RMSE: {final_train_rmse:.6f}\")\n",
    "        print(f\"   â€¢ Hit Rate: {final_train_hit_rate:.3f}\")\n",
    "        print(f\"   â€¢ Sharpe Ratio: {calculate_sharpe_ratio(train_predictions):.4f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Test Set Predictions Generated:\")\n",
    "        print(f\"   â€¢ Number of predictions: {len(test_predictions):,}\")\n",
    "        print(f\"   â€¢ Prediction range: [{test_predictions.min():.6f}, {test_predictions.max():.6f}]\")\n",
    "        print(f\"   â€¢ Mean prediction: {test_predictions.mean():.6f}\")\n",
    "        print(f\"   â€¢ Std prediction: {test_predictions.std():.6f}\")\n",
    "        \n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({\n",
    "            'date_id': df_test['date_id'],\n",
    "            'forward_returns_prediction': test_predictions\n",
    "        })\n",
    "        \n",
    "        # Save predictions\n",
    "        submission_path = '../data/predictions/baseline_predictions.csv'\n",
    "        import os\n",
    "        os.makedirs('../data/predictions', exist_ok=True)\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Predictions saved to: {submission_path}\")\n",
    "        print(f\"   â€¢ Ready for competition submission\")\n",
    "        \n",
    "        # Display sample predictions\n",
    "        print(f\"\\nğŸ“‹ Sample Predictions:\")\n",
    "        print(submission_df.head(10).to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Final model training failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80c79e4",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1f0c7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ BASELINE MODEL SUMMARY\n",
      "============================================================\n",
      "ğŸ¯ Model Selection Results:\n",
      "   â€¢ Total configurations tested: 28\n",
      "   â€¢ Successful runs: 28\n",
      "   â€¢ Best validation RÂ²: -0.0003\n",
      "   â€¢ Best hit rate: 0.552\n",
      "\n",
      "ğŸ† Final Model Performance:\n",
      "   â€¢ Model: Lasso\n",
      "   â€¢ Feature Set: complete_only\n",
      "   â€¢ Training RÂ²: 0.0000\n",
      "   â€¢ Training Hit Rate: 0.539\n",
      "   â€¢ Test predictions: âœ… Generated\n",
      "\n",
      "ğŸ“Š Key Insights:\n",
      "   â€¢ Best performing model type: Lasso\n",
      "   â€¢ Best performing feature set: complete_only\n",
      "   â€¢ Average overfitting: 0.2023\n",
      "   â€¢ Overall performance: ğŸ”´ Poor - Consider feature engineering\n",
      "\n",
      "ğŸš€ Recommended Next Steps:\n",
      "   1. ğŸ” Feature Engineering: Create interaction terms, lags, rolling statistics\n",
      "   2. ğŸ¯ Hyperparameter Tuning: Optimize best performing models\n",
      "   3. ğŸ”„ Ensemble Methods: Combine multiple models for better performance\n",
      "   4. ğŸ“Š Advanced Missing Value Handling: Use domain-specific imputation\n",
      "   5. âš–ï¸ Model Interpretability: Analyze feature importance and predictions\n",
      "\n",
      "============================================================\n",
      "âœ… BASELINE MODEL PIPELINE COMPLETE\n",
      "ğŸ“ˆ READY FOR ADVANCED MODELING PHASE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"ğŸ“‹ BASELINE MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'results_df' in locals() and len(results_df) > 0:\n",
    "    print(f\"ğŸ¯ Model Selection Results:\")\n",
    "    print(f\"   â€¢ Total configurations tested: {len(all_results)}\")\n",
    "    print(f\"   â€¢ Successful runs: {len(results_df)}\")\n",
    "    print(f\"   â€¢ Best validation RÂ²: {results_df['test_r2'].max():.4f}\")\n",
    "    print(f\"   â€¢ Best hit rate: {results_df['hit_rate'].max():.3f}\")\n",
    "    \n",
    "    if 'final_train_r2' in locals():\n",
    "        print(f\"\\nğŸ† Final Model Performance:\")\n",
    "        print(f\"   â€¢ Model: {best_model_info['model_name']}\")\n",
    "        print(f\"   â€¢ Feature Set: {best_model_info['feature_set']}\")\n",
    "        print(f\"   â€¢ Training RÂ²: {final_train_r2:.4f}\")\n",
    "        print(f\"   â€¢ Training Hit Rate: {final_train_hit_rate:.3f}\")\n",
    "        print(f\"   â€¢ Test predictions: âœ… Generated\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Key Insights:\")\n",
    "    best_model_type = results_df.loc[results_df['test_r2'].idxmax(), 'model']\n",
    "    best_feature_set = results_df.loc[results_df['test_r2'].idxmax(), 'feature_set']\n",
    "    print(f\"   â€¢ Best performing model type: {best_model_type}\")\n",
    "    print(f\"   â€¢ Best performing feature set: {best_feature_set}\")\n",
    "    print(f\"   â€¢ Average overfitting: {results_df['overfitting'].mean():.4f}\")\n",
    "    \n",
    "    # Performance interpretation\n",
    "    avg_r2 = results_df['test_r2'].mean()\n",
    "    if avg_r2 > 0.1:\n",
    "        performance_rating = \"ğŸŸ¢ Good\"\n",
    "    elif avg_r2 > 0.0:\n",
    "        performance_rating = \"ğŸŸ¡ Moderate\"\n",
    "    else:\n",
    "        performance_rating = \"ğŸ”´ Poor - Consider feature engineering\"\n",
    "    \n",
    "    print(f\"   â€¢ Overall performance: {performance_rating}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No successful model runs to summarize\")\n",
    "\n",
    "print(f\"\\nğŸš€ Recommended Next Steps:\")\n",
    "print(f\"   1. ğŸ” Feature Engineering: Create interaction terms, lags, rolling statistics\")\n",
    "print(f\"   2. ğŸ¯ Hyperparameter Tuning: Optimize best performing models\")\n",
    "print(f\"   3. ğŸ”„ Ensemble Methods: Combine multiple models for better performance\")\n",
    "print(f\"   4. ğŸ“Š Advanced Missing Value Handling: Use domain-specific imputation\")\n",
    "print(f\"   5. âš–ï¸ Model Interpretability: Analyze feature importance and predictions\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"âœ… BASELINE MODEL PIPELINE COMPLETE\")\n",
    "print(f\"ğŸ“ˆ READY FOR ADVANCED MODELING PHASE\")\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9251999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Performance Comparison with Clean Imputed Data\n",
    "\n",
    "#Now let's test the same best model using the clean `train_imputed.csv` dataset to measure performance improvement from advanced missing value handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d28774e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ PERFORMANCE COMPARISON WITH CLEAN IMPUTED DATA\n",
      "======================================================================\n",
      "âœ… Clean imputed data loaded successfully:\n",
      "   â€¢ Shape: (8990, 100)\n",
      "   â€¢ Missing values: 110,204\n",
      "\n",
      "ğŸ“Š ORIGINAL BASELINE PERFORMANCE:\n",
      "   â€¢ Model: Lasso\n",
      "   â€¢ Feature Set: complete_only\n",
      "   â€¢ Validation RÂ²: -0.0003\n",
      "   â€¢ Training RÂ²: 0.0000\n",
      "   â€¢ Hit Rate: 0.539\n",
      "\n",
      "ğŸ¯ Clean Data Target Analysis:\n",
      "   â€¢ Target: forward_returns\n",
      "   â€¢ Samples: 8,990\n",
      "   â€¢ Missing values: 0\n",
      "   â€¢ Final clean samples: 8,990\n",
      "\n",
      "ğŸ”„ TESTING BEST MODEL WITH CLEAN DATA...\n",
      "--------------------------------------------------\n",
      "ğŸ“Š Strategy 1: Same features as original best model\n",
      "   â€¢ Using 9 features from original best model\n",
      "   â€¢ Missing values in features: 0\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   âœ… Lasso with clean data:\n",
      "      RÂ² = -0.0003\n",
      "      Hit Rate = 0.552\n",
      "      RMSE = 0.011091\n",
      "\n",
      "ğŸ“Š Strategy 2: Use all available clean features\n",
      "   â€¢ Using 96 clean features\n",
      "   â€¢ Missing values: 110204\n",
      "âœ… Clean imputed data loaded successfully:\n",
      "   â€¢ Shape: (8990, 100)\n",
      "   â€¢ Missing values: 110,204\n",
      "\n",
      "ğŸ“Š ORIGINAL BASELINE PERFORMANCE:\n",
      "   â€¢ Model: Lasso\n",
      "   â€¢ Feature Set: complete_only\n",
      "   â€¢ Validation RÂ²: -0.0003\n",
      "   â€¢ Training RÂ²: 0.0000\n",
      "   â€¢ Hit Rate: 0.539\n",
      "\n",
      "ğŸ¯ Clean Data Target Analysis:\n",
      "   â€¢ Target: forward_returns\n",
      "   â€¢ Samples: 8,990\n",
      "   â€¢ Missing values: 0\n",
      "   â€¢ Final clean samples: 8,990\n",
      "\n",
      "ğŸ”„ TESTING BEST MODEL WITH CLEAN DATA...\n",
      "--------------------------------------------------\n",
      "ğŸ“Š Strategy 1: Same features as original best model\n",
      "   â€¢ Using 9 features from original best model\n",
      "   â€¢ Missing values in features: 0\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   âœ… Lasso with clean data:\n",
      "      RÂ² = -0.0003\n",
      "      Hit Rate = 0.552\n",
      "      RMSE = 0.011091\n",
      "\n",
      "ğŸ“Š Strategy 2: Use all available clean features\n",
      "   â€¢ Using 96 clean features\n",
      "   â€¢ Missing values: 110204\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   âœ… Lasso with all clean features:\n",
      "      RÂ² = -0.0003\n",
      "      Hit Rate = 0.552\n",
      "      RMSE = 0.011091\n",
      "\n",
      "ğŸ“ˆ PERFORMANCE IMPROVEMENT ANALYSIS\n",
      "============================================================\n",
      "ğŸ† BEST CLEAN DATA PERFORMANCE:\n",
      "   â€¢ Model: Lasso_Clean\n",
      "   â€¢ Strategy: same_features\n",
      "   â€¢ Features: 9\n",
      "   â€¢ RÂ² Score: -0.0003\n",
      "   â€¢ Hit Rate: 0.552\n",
      "   â€¢ RMSE: 0.011091\n",
      "\n",
      "ğŸ“Š IMPROVEMENT METRICS:\n",
      "   â€¢ RÂ² Improvement: +0.0000\n",
      "   â€¢ Hit Rate Improvement: +0.013\n",
      "   â€¢ Relative RÂ² Improvement: +0.0%\n",
      "   â€¢ Status: ğŸ”´ No Improvement\n",
      "\n",
      "ğŸ”„ ALL STRATEGIES COMPARISON:\n",
      "   1. same_features (9 features):\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "   2. all_clean_features (96 features):\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "\n",
      "âœ… Clean data testing complete!\n",
      "   â€¢ Advanced missing value handling shows improvement\n",
      "   â€¢ Train split: 7,192 samples\n",
      "   â€¢ Validation split: 1,798 samples\n",
      "   âœ… Lasso with all clean features:\n",
      "      RÂ² = -0.0003\n",
      "      Hit Rate = 0.552\n",
      "      RMSE = 0.011091\n",
      "\n",
      "ğŸ“ˆ PERFORMANCE IMPROVEMENT ANALYSIS\n",
      "============================================================\n",
      "ğŸ† BEST CLEAN DATA PERFORMANCE:\n",
      "   â€¢ Model: Lasso_Clean\n",
      "   â€¢ Strategy: same_features\n",
      "   â€¢ Features: 9\n",
      "   â€¢ RÂ² Score: -0.0003\n",
      "   â€¢ Hit Rate: 0.552\n",
      "   â€¢ RMSE: 0.011091\n",
      "\n",
      "ğŸ“Š IMPROVEMENT METRICS:\n",
      "   â€¢ RÂ² Improvement: +0.0000\n",
      "   â€¢ Hit Rate Improvement: +0.013\n",
      "   â€¢ Relative RÂ² Improvement: +0.0%\n",
      "   â€¢ Status: ğŸ”´ No Improvement\n",
      "\n",
      "ğŸ”„ ALL STRATEGIES COMPARISON:\n",
      "   1. same_features (9 features):\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "   2. all_clean_features (96 features):\n",
      "      RÂ² = -0.0003, Hit Rate = 0.552\n",
      "\n",
      "âœ… Clean data testing complete!\n",
      "   â€¢ Advanced missing value handling shows improvement\n"
     ]
    }
   ],
   "source": [
    "# Load the clean imputed training data and compare performance\n",
    "print(\"ğŸ§¹ PERFORMANCE COMPARISON WITH CLEAN IMPUTED DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Load the clean imputed dataset\n",
    "    df_train_imputed = pd.read_csv('../data/cleaned/train_imputed.csv')\n",
    "    print(f\"âœ… Clean imputed data loaded successfully:\")\n",
    "    print(f\"   â€¢ Shape: {df_train_imputed.shape}\")\n",
    "    print(f\"   â€¢ Missing values: {df_train_imputed.isnull().sum().sum():,}\")\n",
    "    \n",
    "    # Store original baseline results for comparison\n",
    "    original_r2 = best_model_info['score']\n",
    "    original_model_name = best_model_info['model_name']\n",
    "    original_feature_set = best_model_info['feature_set']\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ORIGINAL BASELINE PERFORMANCE:\")\n",
    "    print(f\"   â€¢ Model: {original_model_name}\")\n",
    "    print(f\"   â€¢ Feature Set: {original_feature_set}\")\n",
    "    print(f\"   â€¢ Validation RÂ²: {original_r2:.4f}\")\n",
    "    print(f\"   â€¢ Training RÂ²: {final_train_r2:.4f}\")\n",
    "    print(f\"   â€¢ Hit Rate: {final_train_hit_rate:.3f}\")\n",
    "    \n",
    "    # Prepare clean data with same target\n",
    "    y_imputed = df_train_imputed[primary_target].copy()\n",
    "    print(f\"\\nğŸ¯ Clean Data Target Analysis:\")\n",
    "    print(f\"   â€¢ Target: {primary_target}\")\n",
    "    print(f\"   â€¢ Samples: {len(y_imputed):,}\")\n",
    "    print(f\"   â€¢ Missing values: {y_imputed.isnull().sum()}\")\n",
    "    \n",
    "    # Remove any rows with missing targets in clean data\n",
    "    valid_idx_clean = ~y_imputed.isnull()\n",
    "    if valid_idx_clean.sum() < len(y_imputed):\n",
    "        print(f\"   â€¢ Removing {(~valid_idx_clean).sum()} samples with missing targets\")\n",
    "        df_train_imputed_clean = df_train_imputed[valid_idx_clean].copy()\n",
    "        y_imputed = y_imputed[valid_idx_clean].copy()\n",
    "    else:\n",
    "        df_train_imputed_clean = df_train_imputed.copy()\n",
    "    \n",
    "    print(f\"   â€¢ Final clean samples: {len(y_imputed):,}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Clean imputed data file not found!\")\n",
    "    print(\"   â€¢ Please ensure train_imputed.csv exists in ../data/cleaned/\")\n",
    "    print(\"   â€¢ Run the advanced missing value handling notebook first\")\n",
    "    df_train_imputed_clean = None\n",
    "\n",
    "if 'df_train_imputed_clean' in locals() and df_train_imputed_clean is not None:\n",
    "    print(f\"\\nğŸ”„ TESTING BEST MODEL WITH CLEAN DATA...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Test different feature strategies with clean data\n",
    "    clean_results = []\n",
    "    \n",
    "    # Strategy 1: Use same features as original best model\n",
    "    print(f\"ğŸ“Š Strategy 1: Same features as original best model\")\n",
    "    try:\n",
    "        # Get the same features used in original best model\n",
    "        original_features = best_model_info['features']\n",
    "        available_clean_features = [f for f in original_features if f in df_train_imputed_clean.columns]\n",
    "        \n",
    "        if len(available_clean_features) > 0:\n",
    "            print(f\"   â€¢ Using {len(available_clean_features)} features from original best model\")\n",
    "            \n",
    "            # Prepare clean feature data\n",
    "            X_clean = df_train_imputed_clean[available_clean_features].copy()\n",
    "            \n",
    "            # Check for missing values in clean data\n",
    "            clean_missing = X_clean.isnull().sum().sum()\n",
    "            print(f\"   â€¢ Missing values in features: {clean_missing}\")\n",
    "            \n",
    "            if clean_missing > 0:\n",
    "                # Apply same imputation as before for consistency\n",
    "                imputer_clean = SimpleImputer(strategy='median')\n",
    "                X_clean_imputed = pd.DataFrame(\n",
    "                    imputer_clean.fit_transform(X_clean),\n",
    "                    columns=available_clean_features\n",
    "                )\n",
    "                X_clean_imputed = X_clean_imputed.fillna(method='ffill').fillna(0)\n",
    "            else:\n",
    "                X_clean_imputed = X_clean.copy()\n",
    "            \n",
    "            # Scale features\n",
    "            scaler_clean = RobustScaler()\n",
    "            X_clean_scaled = scaler_clean.fit_transform(X_clean_imputed)\n",
    "            \n",
    "            # Create train/validation split maintaining time order\n",
    "            split_idx_clean = int(len(X_clean_scaled) * 0.8)\n",
    "            \n",
    "            X_train_clean_split = X_clean_scaled[:split_idx_clean]\n",
    "            X_val_clean_split = X_clean_scaled[split_idx_clean:]\n",
    "            y_train_clean_split = y_imputed.iloc[:split_idx_clean]\n",
    "            y_val_clean_split = y_imputed.iloc[split_idx_clean:]\n",
    "            \n",
    "            print(f\"   â€¢ Train split: {len(X_train_clean_split):,} samples\")\n",
    "            print(f\"   â€¢ Validation split: {len(X_val_clean_split):,} samples\")\n",
    "            \n",
    "            # Test the same model type as original best\n",
    "            from sklearn.base import clone\n",
    "            clean_model = clone(best_model_info['model'])\n",
    "            \n",
    "            # Evaluate model with clean data\n",
    "            clean_result = evaluate_model(\n",
    "                clean_model, X_train_clean_split, y_train_clean_split,\n",
    "                X_val_clean_split, y_val_clean_split, f\"{original_model_name}_Clean\"\n",
    "            )\n",
    "            clean_result['strategy'] = 'same_features'\n",
    "            clean_result['n_features'] = len(available_clean_features)\n",
    "            clean_results.append(clean_result)\n",
    "            \n",
    "            print(f\"   âœ… {original_model_name} with clean data:\")\n",
    "            print(f\"      RÂ² = {clean_result['test_r2']:.4f}\")\n",
    "            print(f\"      Hit Rate = {clean_result['hit_rate']:.3f}\")\n",
    "            print(f\"      RMSE = {clean_result['test_rmse']:.6f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   âŒ No matching features found in clean data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Strategy 1 failed: {str(e)[:100]}\")\n",
    "    \n",
    "    # Strategy 2: Use all available features from clean data (no missing values)\n",
    "    print(f\"\\nğŸ“Š Strategy 2: Use all available clean features\")\n",
    "    try:\n",
    "        # Get all feature columns from clean data (excluding targets and IDs)\n",
    "        clean_feature_cols = [col for col in df_train_imputed_clean.columns if col not in exclude_cols]\n",
    "        \n",
    "        if len(clean_feature_cols) > 0:\n",
    "            print(f\"   â€¢ Using {len(clean_feature_cols)} clean features\")\n",
    "            \n",
    "            # Prepare feature data\n",
    "            X_all_clean = df_train_imputed_clean[clean_feature_cols].copy()\n",
    "            \n",
    "            # Check for missing values\n",
    "            all_clean_missing = X_all_clean.isnull().sum().sum()\n",
    "            print(f\"   â€¢ Missing values: {all_clean_missing}\")\n",
    "            \n",
    "            if all_clean_missing > 0:\n",
    "                # Light imputation for any remaining missing values\n",
    "                imputer_all = SimpleImputer(strategy='median')\n",
    "                X_all_clean_processed = pd.DataFrame(\n",
    "                    imputer_all.fit_transform(X_all_clean),\n",
    "                    columns=clean_feature_cols\n",
    "                )\n",
    "                X_all_clean_processed = X_all_clean_processed.fillna(method='ffill').fillna(0)\n",
    "            else:\n",
    "                X_all_clean_processed = X_all_clean.copy()\n",
    "            \n",
    "            # Scale features\n",
    "            scaler_all = RobustScaler()\n",
    "            X_all_clean_scaled = scaler_all.fit_transform(X_all_clean_processed)\n",
    "            \n",
    "            # Create train/validation split\n",
    "            split_idx_all = int(len(X_all_clean_scaled) * 0.8)\n",
    "            \n",
    "            X_train_all_split = X_all_clean_scaled[:split_idx_all]\n",
    "            X_val_all_split = X_all_clean_scaled[split_idx_all:]\n",
    "            y_train_all_split = y_imputed.iloc[:split_idx_all]\n",
    "            y_val_all_split = y_imputed.iloc[split_idx_all:]\n",
    "            \n",
    "            print(f\"   â€¢ Train split: {len(X_train_all_split):,} samples\")\n",
    "            print(f\"   â€¢ Validation split: {len(X_val_all_split):,} samples\")\n",
    "            \n",
    "            # Test same model type with all clean features\n",
    "            clean_model_all = clone(best_model_info['model'])\n",
    "            \n",
    "            clean_result_all = evaluate_model(\n",
    "                clean_model_all, X_train_all_split, y_train_all_split,\n",
    "                X_val_all_split, y_val_all_split, f\"{original_model_name}_AllClean\"\n",
    "            )\n",
    "            clean_result_all['strategy'] = 'all_clean_features'\n",
    "            clean_result_all['n_features'] = len(clean_feature_cols)\n",
    "            clean_results.append(clean_result_all)\n",
    "            \n",
    "            print(f\"   âœ… {original_model_name} with all clean features:\")\n",
    "            print(f\"      RÂ² = {clean_result_all['test_r2']:.4f}\")\n",
    "            print(f\"      Hit Rate = {clean_result_all['hit_rate']:.3f}\")\n",
    "            print(f\"      RMSE = {clean_result_all['test_rmse']:.6f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   âŒ No feature columns found in clean data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Strategy 2 failed: {str(e)[:100]}\")\n",
    "    \n",
    "    # Performance comparison summary\n",
    "    print(f\"\\nğŸ“ˆ PERFORMANCE IMPROVEMENT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(clean_results) > 0:\n",
    "        # Best clean result\n",
    "        best_clean = max(clean_results, key=lambda x: x['test_r2'])\n",
    "        \n",
    "        print(f\"ğŸ† BEST CLEAN DATA PERFORMANCE:\")\n",
    "        print(f\"   â€¢ Model: {best_clean['model']}\")\n",
    "        print(f\"   â€¢ Strategy: {best_clean['strategy']}\")\n",
    "        print(f\"   â€¢ Features: {best_clean['n_features']}\")\n",
    "        print(f\"   â€¢ RÂ² Score: {best_clean['test_r2']:.4f}\")\n",
    "        print(f\"   â€¢ Hit Rate: {best_clean['hit_rate']:.3f}\")\n",
    "        print(f\"   â€¢ RMSE: {best_clean['test_rmse']:.6f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š IMPROVEMENT METRICS:\")\n",
    "        r2_improvement = best_clean['test_r2'] - original_r2\n",
    "        hit_rate_improvement = best_clean['hit_rate'] - final_train_hit_rate\n",
    "        \n",
    "        print(f\"   â€¢ RÂ² Improvement: {r2_improvement:+.4f}\")\n",
    "        print(f\"   â€¢ Hit Rate Improvement: {hit_rate_improvement:+.3f}\")\n",
    "        \n",
    "        if r2_improvement > 0:\n",
    "            relative_improvement = (r2_improvement / abs(original_r2)) * 100 if original_r2 != 0 else float('inf')\n",
    "            print(f\"   â€¢ Relative RÂ² Improvement: {relative_improvement:+.1f}%\")\n",
    "        else:\n",
    "            print(f\"   â€¢ Relative RÂ² Improvement: {r2_improvement/abs(original_r2)*100:+.1f}%\" if original_r2 != 0 else \"N/A\")\n",
    "        \n",
    "        # Performance interpretation\n",
    "        if r2_improvement > 0.01:\n",
    "            improvement_status = \"ğŸŸ¢ Significant Improvement\"\n",
    "        elif r2_improvement > 0:\n",
    "            improvement_status = \"ğŸŸ¡ Modest Improvement\"\n",
    "        else:\n",
    "            improvement_status = \"ğŸ”´ No Improvement\"\n",
    "        \n",
    "        print(f\"   â€¢ Status: {improvement_status}\")\n",
    "        \n",
    "        # All strategies comparison\n",
    "        if len(clean_results) > 1:\n",
    "            print(f\"\\nğŸ”„ ALL STRATEGIES COMPARISON:\")\n",
    "            for i, result in enumerate(clean_results, 1):\n",
    "                print(f\"   {i}. {result['strategy']} ({result['n_features']} features):\")\n",
    "                print(f\"      RÂ² = {result['test_r2']:.4f}, Hit Rate = {result['hit_rate']:.3f}\")\n",
    "        \n",
    "        # Store best clean model info for potential final training\n",
    "        best_clean_model_info = {\n",
    "            'model': best_clean['model'] if 'model' not in best_clean or isinstance(best_clean['model'], str) else clone(best_model_info['model']),\n",
    "            'strategy': best_clean['strategy'],\n",
    "            'features': clean_feature_cols if best_clean['strategy'] == 'all_clean_features' else available_clean_features,\n",
    "            'score': best_clean['test_r2'],\n",
    "            'data': 'train_imputed.csv'\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nâœ… Clean data testing complete!\")\n",
    "        print(f\"   â€¢ Advanced missing value handling shows {improvement_status.split()[-1].lower()}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âŒ No clean data results to compare\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Skipping clean data comparison - imputed dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Train final model with clean data and generate improved test predictions\n",
    "print(\"ğŸ¯ FINAL MODEL TRAINING WITH CLEAN DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'best_clean_model_info' in locals() and 'df_train_imputed_clean' in locals():\n",
    "    # Check if clean data shows improvement\n",
    "    if best_clean_model_info['score'] > original_r2:\n",
    "        print(f\"ğŸš€ Clean data shows improvement - training final model with clean data\")\n",
    "        print(f\"   â€¢ Improvement: {best_clean_model_info['score'] - original_r2:+.4f} RÂ²\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare final clean training data\n",
    "            X_final_clean = df_train_imputed_clean[best_clean_model_info['features']].copy()\n",
    "            y_final_clean = y_imputed.copy()\n",
    "            \n",
    "            # Prepare test data with same features\n",
    "            test_features_available = [f for f in best_clean_model_info['features'] if f in df_test.columns]\n",
    "            X_test_final_clean = df_test[test_features_available].copy()\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Final Clean Data Preparation:\")\n",
    "            print(f\"   â€¢ Training samples: {len(X_final_clean):,}\")\n",
    "            print(f\"   â€¢ Test samples: {len(X_test_final_clean):,}\")\n",
    "            print(f\"   â€¢ Features: {len(test_features_available)}\")\n",
    "            \n",
    "            # Handle any missing values in clean data\n",
    "            clean_missing_final = X_final_clean.isnull().sum().sum()\n",
    "            test_missing_final = X_test_final_clean.isnull().sum().sum()\n",
    "            \n",
    "            if clean_missing_final > 0 or test_missing_final > 0:\n",
    "                print(f\"   â€¢ Handling missing values: Train={clean_missing_final}, Test={test_missing_final}\")\n",
    "                imputer_final = SimpleImputer(strategy='median')\n",
    "                X_final_clean = pd.DataFrame(\n",
    "                    imputer_final.fit_transform(X_final_clean),\n",
    "                    columns=best_clean_model_info['features']\n",
    "                )\n",
    "                X_test_final_clean = pd.DataFrame(\n",
    "                    imputer_final.transform(X_test_final_clean),\n",
    "                    columns=test_features_available\n",
    "                )\n",
    "                # Handle any remaining missing values\n",
    "                X_final_clean = X_final_clean.fillna(method='ffill').fillna(0)\n",
    "                X_test_final_clean = X_test_final_clean.fillna(method='ffill').fillna(0)\n",
    "            \n",
    "            # Scale features\n",
    "            scaler_final = RobustScaler()\n",
    "            X_final_clean_scaled = scaler_final.fit_transform(X_final_clean)\n",
    "            X_test_final_clean_scaled = scaler_final.transform(X_test_final_clean)\n",
    "            \n",
    "            print(f\"   â€¢ Preprocessing complete\")\n",
    "            \n",
    "            # Train final model on full clean training data\n",
    "            final_clean_model = clone(best_model_info['model'])\n",
    "            print(f\"\\nğŸš€ Training final clean model...\")\n",
    "            final_clean_model.fit(X_final_clean_scaled, y_final_clean)\n",
    "            \n",
    "            # Make predictions\n",
    "            clean_train_predictions = final_clean_model.predict(X_final_clean_scaled)\n",
    "            clean_test_predictions = final_clean_model.predict(X_test_final_clean_scaled)\n",
    "            \n",
    "            # Calculate final clean metrics\n",
    "            final_clean_r2 = r2_score(y_final_clean, clean_train_predictions)\n",
    "            final_clean_rmse = np.sqrt(mean_squared_error(y_final_clean, clean_train_predictions))\n",
    "            final_clean_hit_rate = calculate_hit_rate(y_final_clean, clean_train_predictions)\n",
    "            \n",
    "            print(f\"\\nâœ… FINAL CLEAN MODEL PERFORMANCE:\")\n",
    "            print(f\"=\" * 50)\n",
    "            print(f\"ğŸ“Š Training Set Performance:\")\n",
    "            print(f\"   â€¢ RÂ² Score: {final_clean_r2:.4f}\")\n",
    "            print(f\"   â€¢ RMSE: {final_clean_rmse:.6f}\")\n",
    "            print(f\"   â€¢ Hit Rate: {final_clean_hit_rate:.3f}\")\n",
    "            print(f\"   â€¢ Sharpe Ratio: {calculate_sharpe_ratio(clean_train_predictions):.4f}\")\n",
    "            \n",
    "            print(f\"\\nğŸ¯ Clean Test Set Predictions Generated:\")\n",
    "            print(f\"   â€¢ Number of predictions: {len(clean_test_predictions):,}\")\n",
    "            print(f\"   â€¢ Prediction range: [{clean_test_predictions.min():.6f}, {clean_test_predictions.max():.6f}]\")\n",
    "            print(f\"   â€¢ Mean prediction: {clean_test_predictions.mean():.6f}\")\n",
    "            print(f\"   â€¢ Std prediction: {clean_test_predictions.std():.6f}\")\n",
    "            \n",
    "            # Create improved submission DataFrame\n",
    "            improved_submission_df = pd.DataFrame({\n",
    "                'date_id': df_test['date_id'],\n",
    "                'forward_returns_prediction': clean_test_predictions\n",
    "            })\n",
    "            \n",
    "            # Save improved predictions\n",
    "            improved_submission_path = '../data/predictions/baseline_predictions_improved.csv'\n",
    "            improved_submission_df.to_csv(improved_submission_path, index=False)\n",
    "            \n",
    "            print(f\"\\nğŸ’¾ Improved predictions saved to: {improved_submission_path}\")\n",
    "            print(f\"   â€¢ Ready for competition submission\")\n",
    "            \n",
    "            # Compare predictions\n",
    "            print(f\"\\nğŸ“‹ Prediction Comparison (First 10 samples):\")\n",
    "            comparison_df = pd.DataFrame({\n",
    "                'date_id': df_test['date_id'].head(10),\n",
    "                'original_prediction': test_predictions[:10],\n",
    "                'improved_prediction': clean_test_predictions[:10],\n",
    "                'difference': clean_test_predictions[:10] - test_predictions[:10]\n",
    "            })\n",
    "            print(comparison_df.to_string(index=False, float_format='%.6f'))\n",
    "            \n",
    "            # Final improvement summary\n",
    "            print(f\"\\nğŸ† FINAL IMPROVEMENT SUMMARY:\")\n",
    "            print(f\"=\" * 50)\n",
    "            print(f\"ğŸ“ˆ Training Performance Comparison:\")\n",
    "            print(f\"   â€¢ Original RÂ²: {final_train_r2:.4f}\")\n",
    "            print(f\"   â€¢ Improved RÂ²: {final_clean_r2:.4f}\")\n",
    "            print(f\"   â€¢ RÂ² Improvement: {final_clean_r2 - final_train_r2:+.4f}\")\n",
    "            print(f\"   â€¢ Original Hit Rate: {final_train_hit_rate:.3f}\")\n",
    "            print(f\"   â€¢ Improved Hit Rate: {final_clean_hit_rate:.3f}\")\n",
    "            print(f\"   â€¢ Hit Rate Improvement: {final_clean_hit_rate - final_train_hit_rate:+.3f}\")\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Test Prediction Analysis:\")\n",
    "            prediction_change = np.abs(clean_test_predictions - test_predictions).mean()\n",
    "            print(f\"   â€¢ Average prediction change: {prediction_change:.6f}\")\n",
    "            print(f\"   â€¢ Max prediction change: {np.abs(clean_test_predictions - test_predictions).max():.6f}\")\n",
    "            \n",
    "            if final_clean_r2 > final_train_r2:\n",
    "                improvement_magnitude = \"ğŸŸ¢ Significant\" if (final_clean_r2 - final_train_r2) > 0.01 else \"ğŸŸ¡ Modest\"\n",
    "                print(f\"   â€¢ Overall improvement: {improvement_magnitude}\")\n",
    "                print(f\"   â€¢ Recommendation: Use improved predictions for submission\")\n",
    "            else:\n",
    "                print(f\"   â€¢ Overall improvement: ğŸ”´ No significant improvement\")\n",
    "                print(f\"   â€¢ Recommendation: Review feature engineering approach\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Final clean model training failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    else:\n",
    "        print(f\"ğŸ“Š Clean data does not show significant improvement over original baseline\")\n",
    "        print(f\"   â€¢ Original RÂ²: {original_r2:.4f}\")\n",
    "        print(f\"   â€¢ Best clean RÂ²: {best_clean_model_info['score']:.4f}\")\n",
    "        print(f\"   â€¢ Difference: {best_clean_model_info['score'] - original_r2:+.4f}\")\n",
    "        print(f\"   â€¢ Recommendation: Focus on feature engineering and advanced modeling\")\n",
    "\n",
    "else:\n",
    "    print(f\"âš ï¸ Clean data comparison not available - using original baseline results\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"âœ… COMPLETE BASELINE VS CLEAN DATA ANALYSIS FINISHED\")\n",
    "print(f\"ğŸ“ˆ READY FOR ADVANCED FEATURE ENGINEERING\")\n",
    "print(f\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
